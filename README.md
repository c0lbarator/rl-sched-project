# Описание проекта

В этом репозитории лежит код для проекта.

## Постановка задачи

### Среда (Environment)
Наша среда — это GraphRunner и сам вычислительный граф.

### Состояние (State)
Информация, на основе которой агент принимает решение. Это может быть:
- Характеристики текущего узла графа (тип операции, ожидаемое время выполнения на CPU/GPU)
- Состояние зависимостей узла (готовы ли входные данные)
- Общее состояние графа (сколько узлов осталось, сколько уже запланировано)

**Упрощение:**  
Начнем с простого: Состояние будет представлять характеристики одного конкретного узла, который мы рассматриваем для планирования.

### Действие (Action)
Решение, которое принимает агент для конкретного узла. В нашем случае это:
- **Приоритет (Priority):** Числовое значение. Чем выше, тем раньше StarPU постарается выполнить задачу.
- **Сродство к устройству (Device Affinity):** На каких типах устройств (CPU/GPU) может выполняться задача. Агент может выбрать:
  - CPU
  - GPU
  - Оба типа (mixed)

**Упрощение:**  
Начнем с того, что агент для каждого узла выбирает предпочтительный тип устройства (CPU или GPU). Приоритет будем пока назначать по простой схеме (например, как в dynamic, но с учетом выбора устройства).

### Награда (Reward)
Сигнал обратной связи, показывающий, насколько хорошо агент справился. Логично использовать отрицательное время выполнения всего графа (-timings). Чем меньше время, тем больше награда.

### Компоненты обучения
- **Политика (Policy - Actor):** Нейронная сеть, которая по состоянию предсказывает распределение вероятностей для действий (например, вероятность выбора CPU или GPU).
- **Оценщик ценности (Value Function - Critic):** Нейронная сеть, которая по состоянию предсказывает ожидаемую будущую награду (насколько "хорошо" текущее состояние).

### Алгоритм PPO
Использует Actor и Critic для обновления политики таким образом, чтобы максимизировать награду, избегая слишком больших изменений политики на каждом шаге (отсюда "Proximal").

## Следующие шаги

1. **Интеграция с `AgentTrainer`:**
   - Нужно создать `AgentTrainer` (или адаптировать существующий), который будет управлять циклом обучения.
   - `AgentTrainer.step` будет получать `graph`, `timings`, `done` от `Runner`.
   - В `step` нужно будет вычислить награду (например, `-makespan` или `1 / makespan`).
   - Данные из `agent.episode_buffer` (states, actions, log_probs, values) и вычисленная награда для *каждого* шага (узла) должны быть добавлены в `PPOBuffer`. Важно правильно обработать `done` (маркер конца графа/эпизода).
   - Когда `PPOBuffer` накопит достаточно данных (например, `update_timestep` шагов), вызвать `agent.update(ppo_buffer)` и очистить буфер (`ppo_buffer.clear()`).

2. **Вычисление награды:**
   Определить, как вычисляется награда. Варианты:
   - Простейший: одна награда для всего графа в конце, равная `-makespan`
   - Более сложный: "reward shaping", где промежуточные шаги получают награду (например, на основе अनुमानित времени выполнения), но это сложнее и может привести к неоптимальным локальным решениям.
   
   **Начнем с награды в конце эпизода.**

3. **Определение `state_dim`:**
   Убедиться, что `state_dim` в конструкторе агента соответствует размеру вектора, возвращаемого `_get_state`. Сейчас это 6.

4. **Гиперпараметры:**
   Настроить гиперпараметры PPO:
   - `lr_actor`
   - `lr_critic`
   - `gamma`
   - `K_epochs`
   - `eps_clip`
   - размер буфера
   - `update_timestep`

5. **Логирование:**
   Добавить логирование (например, с помощью TensorBoard или `wandb`) для отслеживания:
   - наград
   - потерь
   - makespan
   - других метрик
